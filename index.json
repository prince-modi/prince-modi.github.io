
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I’m an incoming Master’s student in Computer Science and Engineering at UC San Diego, specializing in Distributed Systems. I have hands-on experience working on scalable and resilient systems, including development of Flotilla, a modular federated learning framework, during my time at the Indian Institute of Science. While working on Flotilla, I became especially interested in the challenges of fault tolerance, consistency, and recovery in distributed systems, areas I’m excited to explore further. My broader interests span distributed computing, systems programming, and computer networks.\nExperience Research Associate, DREAM:Lab, IISc Bengaluru (2023–2024) Software Engineering Intern, Sterlite Technologies Ltd., Ahmedabad (2022) Education MS-CSE Distributed Systems, UC San Diego (2025–2027) B.Tech Computer Engineering, Ganpat University - UVPCE (2018–2022) ","date":1702857600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1702857600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"2023-12-18T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m an incoming Master’s student in Computer Science and Engineering at UC San Diego, specializing in Distributed Systems. I have hands-on experience working on scalable and resilient systems, including development of Flotilla, a modular federated learning framework, during my time at the Indian Institute of Science.","tags":null,"title":"Prince Modi","type":"authors"},{"authors":null,"categories":null,"content":"\rThe paper The Google File System by Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung (Google) presents details of a distributed file system designed to support distributed applications and the various assumptions and design decisions that were made.\nCredit: google.com\nIntroduction From the introduction of the paper, we have the following:\nWe have designed and implemented the Google File System (GFS) to meet the rapidly growing demands of Google’s data processing needs. GFS shares many of the same goals as previous distributed file systems such as performance, scalability, reliability, and availability. However, its design has been driven by key observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system design assumptions.\nThe decisions made while designing and implementing GFS mainly focused on the fact that there was a need for a scalable distributed file system that would be used in conjunction with large distributed data-intensive applications, for example, MapReduce.\nThe designers understood that on a file system distributed across hundreds or even thousands of machines, the chances of some computers failing would be significantly higher, so they designed the GFS with monitoring, error detection, fault tolerance, and recovery in mind.\nSince the file system is also designed for large data-intensive applications, the files that they would have to store would be larger than the traditional standards.\nIt was found that the majority of the writes that the applications performed were appending the new data at the end of the file. Random writes on files were practically non-existent. Once the data was written, it was only read and that too was often only sequential reads.\nAssumptions There are several core assumptions that constrain the design of GFS, and they are centered around the typical workload of GFS clients. The in-detail assumptions are as follows:\nThe system is built from many inexpensive commodity components that often fail.\nThe assumption is that on a distributed file system operating at such a large scale, the chances that at any given time some components might not be functional is really high, so they have incorporated constant monitoring, fault detection, tolerance, and recovery into the design.\nThe system stores a modest number of large files.\nWe expect a few million files, each typically 100 MB or larger in size. Multi-GB files are the common case and should be managed efficiently. Small files must be supported, but we need not optimize for them.\nGFS is designed to store a large number of large files (\u0026gt;100 MB) consisting of 64 MB chunks.\nThe workloads primarily consist of two kinds of reads:\nLarge streaming reads (hundreds of KBs to MBs). Successive operations from the same client often read through a contiguous region of a file. Small random reads, typically reading a few KBs at some arbitrary offset. Performance-conscious applications often batch and sort their small reads to advance steadily through the file rather than go back and forth. The workloads also have many large, sequential writes that append data to files.\nTypical operation sizes are similar to those for reads. Once written, files are seldom modified again. Small writes at arbitrary positions in a file are supported but do not have to be efficient.\nThe system must efficiently implement well-defined semantics for multiple clients that concurrently append to the same file.\nGFS files will typically experience highly concurrent append-heavy workloads, and it’s crucial these operations are implemented by the system efficiently.\nHigh sustained bandwidth is more important than low latency.\nClients tend to engage in large data processing jobs, which rely on maintaining sustained high bandwidth, more so than optimizing for individual operation latency.\nInterface GFS provides an interface that supports all the usual operations that are expected from a file system such as creating, deleting, opening, closing, reading, and writing files.\nApart from this, GFS also provides snapshot and record append operations. Snapshot creates a copy of a file or a directory tree at a low cost. Record append allows multiple clients to append data in the same file concurrently while ensuring the atomicity of the append operation.\nArchitecture As mentioned earlier, GFS runs on a typical commodity Linux machine. A typical GFS consists of a single master and multiple chunkservers and is accessed by multiple clients. Chunkservers and clients can run on the same machines as long as the resources available on the machine permit that.\nFiles are divided into fixed-size chunks (typically 64 MB). Each of the chunks has a 64-bit chunk handle, assigned to it by the master, which is used for identifying the chunk. For reliability, each of the chunks is replicated on 3 chunkservers by default.\nThe metadata for the whole file system is maintained by the master in …","date":1714780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714780800,"objectID":"f8a658c60ba5f55bf66bf9a1cd49604f","permalink":"http://localhost:1313/post/google-file-system/","publishdate":"2024-05-04T00:00:00Z","relpermalink":"/post/google-file-system/","section":"post","summary":"*An overview of the Google File System (GFS), its architecture, design decisions, and the assumptions behind it, based on the paper by Ghemawat et al.*","tags":["distributed systems","storage","Google File System","research"],"title":"Exploring the Power of Google File System: The Distributed Storage System That Revolutionized Big Data","type":"post"},{"authors":null,"categories":null,"content":"This article is my understanding of distributed transactions and various aspects of Spanner, such as its structure, how it handles transactions like Read-Write and Read-Only Transactions, and the TrueTime API.\nDistributed Transactions Transactions package multiple operations on data records to ensure they execute as a single unit, even in the event of failure. These guarantees are often referred to as ACID properties:\nAtomic: All-or-nothing Consistent: The system must remain in a valid state before and after Isolated: Each transaction appears to run alone Durable: Changes persist despite failures Need for Distributed Transactions Large databases often contain millions of records and are sharded across machines (e.g., one shard contains rows A–N, another O–Z) to improve performance. When a transaction spans multiple shards, we need concurrency control and commit protocols to preserve ACID guarantees.\nSpanner addresses this by using Two-Phase Locking (2PL) and Two-Phase Commit (2PC).\nSpanner Architecture A Spanner deployment is called a universe. It is composed of multiple zones, each with:\nA zonemaster that assigns data to spanservers Between 100 and several thousand spanservers Each spanserver manages 100–1000 tablets, which are sharded partitions of tables based on primary keys.\nTo support replication, Spanner:\nImplements a Paxos state machine per tablet Replicates tablets across spanservers using Paxos groups Handles concurrency control via a lock table at the Paxos leader Uses a transaction manager to coordinate multi-tablet transactions TrueTime API Spanner relies on the TrueTime API to support external consistency and concurrency control.\nTrueTime returns an interval: TTinterval = [earliest, latest] TT.now() returns the interval during which the call occurred TT.after(t) returns true if t has definitely passed TT.before(t) returns true if t has definitely not yet occurred TrueTime uses GPS and atomic clocks, each with different failure modes. Each data center has time master machines, and each machine runs a timeslave daemon that polls these masters.\nIn production, the uncertainty bound ε (epsilon) is typically 1–7 ms, representing half the width of the TTinterval.\nTransactions in Spanner Read-Write Transactions Spanner uses Two-Phase Locking (2PL) and Two-Phase Commit (2PC) for distributed read-write transactions.\nThe coordinator gathers: Prepared timestamps from non-coordinators TTcommit, the commit time from the client It then chooses a commit timestamp that is: Greater than all prepared timestamps Greater than TTcommit.latest Greater than any earlier transaction timestamps Spanner performs a commit wait to ensure this timestamp is safely in the past before finalizing the commit.\nRead-Only Transactions If all required keys reside within a single Paxos group, the leader assigns the last committed write timestamp as the transaction timestamp, minimizing wait time. If keys span multiple Paxos groups, the timestamp is set to TT.now().latest, requiring a small delay until this time safely passes. Read-only transactions can be served from sufficiently up-to-date replicas, allowing for non-blocking and lock-free reads.\nSummary Spanner blends concepts from database and distributed systems research:\nFrom databases: SQL-like interface, relational schema, transactions From systems: Scalability, fault tolerance, sharding, replication, and global distribution Thanks to the TrueTime API, Spanner achieves strong guarantees around external consistency, lock-free read-only transactions, and non-blocking reads in the past—demonstrating that precise time semantics are practical and powerful in distributed systems.\n","date":1709596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709596800,"objectID":"aa92155676e7639f903c097fbfd92a6a","permalink":"http://localhost:1313/post/google-spanner/","publishdate":"2024-03-05T00:00:00Z","relpermalink":"/post/google-spanner/","section":"post","summary":"*An overview of Google Spanner’s architecture, distributed transactions, and the TrueTime API, based on the paper by Corbett et al.*","tags":["distributed systems","databases","Google Spanner","research"],"title":"Exploring the Power of Google Spanner for Distributed Transaction Processing","type":"post"},{"authors":["Roopkatha Banerjee","Prince Modi","Harsha Varun Marisetty","Manik Gupta","Yogesh Simmhan"],"categories":null,"content":"Framework Architecture Press “Towards a Modular Federated Learning Framework on Edge Devices” at HiPC 2023 Student Research Symposium ","date":1702857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702857600,"objectID":"cacc10de9aa074c6dd1ef880d3e6484e","permalink":"http://localhost:1313/publication/conference-poster/hipc-srs-2023/","publishdate":"2023-12-18T00:00:00Z","relpermalink":"/publication/conference-poster/hipc-srs-2023/","section":"publication","summary":"In this poster, we introduce *Flotilla*, a modular, model-agnostic Federated Learning (FL) framework that supports synchronous client-selectoin and agreegatoin strategies, and FL model deployment and training on edge client clusters, with telemetry for advanced systems research.","tags":["Federated Learning","Distributed Systems","Applied Machine Learning"],"title":"Towards a Modular Federated Learning Framework on Edge Devices","type":"publication"},{"authors":["Suman Raj","Swapnil Padhi","Ruchi Bhoot","Prince Modi","Yogesh Simmhan"],"categories":null,"content":"\r","date":1691625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691625600,"objectID":"a679face69ba3275de181538718d55c6","permalink":"http://localhost:1313/publication/conference-poster/iros-lb-2023/","publishdate":"2023-08-10T00:00:00Z","relpermalink":"/publication/conference-poster/iros-lb-2023/","section":"publication","summary":"In this poster, we explore a mechanism to detect obstacles within a distance 'd' ahead of a visually imparied person (VIP) and offer them a path with a minimum width 'w' to navigate between the obstacles.","tags":["Drones","Distributed Systems","Applied Machine Learning, CNNs"],"title":"Towards Collision Avoidance for UAVs to Guide the Visually Impaired","type":"publication"},{"authors":null,"categories":null,"content":" For a review on household displacement and return after disasters, please refer to my open access paper Overview Over 265 million people were displaced due to disasters between 2008 and 20181. In the forthcoming years, the annual number displaced is expected to increase, driven by poorly-managed urban growth in hazard-prone areas2 and potentially exacerbated by climate change3. Despite this scale of human impact, most disaster risk assessments focus on direct economic losses, a metric that often highlights the wealthiest as the most at-risk. However, the reality of disasters is that the poor are disproportionately affected4, and mitigations informed primarily by economic loss may deepen existing inequalities. This research proposes to quantify disaster-induced displacement; a more equitable risk metric to depict the human toll of disasters.\nKey definitions and the scope. The highlighted labels indicate the areas considered in this review. Labels below the highlighted labels are subsets of that category. Research themes The importance of duration Most statistics regarding population displacement following a disaster event provide single snapshot values, often representing a peak estimate during the emergency phase. However, the duration of displacement is essential for understanding the human impact. For example, large- scale displacement in the form of evacuations before a storm can save lives and be followed by mass return shortly afterward. In contrast, a devastating event such as an earthquake could damage or destroy a significant proportion of the residential building stock, causing occupants to seek alternative accommodations for months to years. Not only does this type of protracted displacement pose a significant disruption to the livelihoods of affected households (e.g., lost income, interrupted education), but the consequences can ripple out into the larger community (e.g., outmigration and urban blight, lost economic production). Therefore, a key objective of this research is to refine our understanding of household displacement duration in disasters.\nTimeline representing displacement duration alongside key phases of disaster management and recovery. Determinants of household return Disasters are life events that can subject households to key decision points, such as: whether to evacuate, where to seek shelter, whether to return/wait/relocate, and whether to stay or resettle. From a literature review of household return after disasters, the following categories of determinants have been identified.\nCategory Determinants of return Physical damage to the built environment Habitability of housing (damage, weather, utilities)Housing typeCommunity damageReconstruction progress Psychological \u0026amp; social phenomena Acceleration of ongoing trendsAttachment to placeSocial capital (networks, family and friends)Perceived risk Household demographics Socioeconomic status (e.g., income level)Housing and land tenureRace/ethnicity/casteAge Pre- and post-disaster policies Pre-existing housing conditions (e.g., vacancies)Housing reconstruction approachOther disaster assistance policies The role of housing damage For more on the role of housing damage in population displacement predictions, please refer to my open access paper Disaster literature offers a clear consensus that housing damage is a primary driver of household displacement of disasters, both for initial displacement and longer-term displacement. However, additional factors (e.g., place attachment and housing tenure) have more recently been proposed as highly influential for household return in the recovery phase. Despite the range of factors beyond damage that have been proposed to influence household return, standard practice in disaster risk analysis is to solely consider housing damage. That is, the number of destroyed homes is multiplied by the average household size to yield an estimate of the displaced population.\nAn illustration of the conventional practice for estimating population displacement after disaster events. I benchmarked predictions of household displacement based solely on housing damage to understand the extent to which such simplified models can explain the phenomenon. The scenario model estimates showed some promise to predict potential long-term housing needs. However, quantifying displacement duration remained a clear challenge as official reports lacked this information and model estimates similarly lacked a time component. Mobile location data could theoretically fill the data gap on duration, but the benchmarking results indicate that further investigation is required on such data-driven methods.\nBenchmarking results for displacement estimates using a scenario risk analysis that only considers housing damage (green) versus official reports and mobile location data-based estimates. The full results of the benchmarking study are available in a journal paper and a conference paper.\nPredicting displacement durations For more on …","date":1664150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664150400,"objectID":"2320825263cfcd6dbeaf26dc1e53a2d4","permalink":"http://localhost:1313/project/household-displacement/","publishdate":"2022-09-26T00:00:00Z","relpermalink":"/project/household-displacement/","section":"project","summary":"*Supervised by Prof. Carmine Galasso and Prof. Jack Baker*","tags":["household displacement"],"title":"Household displacement after disasters","type":"project"},{"authors":null,"categories":null,"content":"For more information on the Global Earthquake Model (GEM) Foundation’s Global Seismic Risk model, please refer to their Products page.\n","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"1c6a1facc66aeb539a00f6b9f408fe83","permalink":"http://localhost:1313/project/global-earthquake-model/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/global-earthquake-model/","section":"project","summary":"*Supervised by Prof. Vitor Silva at the GEM Foundation*","tags":["earthquakes","global earthquake model"],"title":"Modeling global earthquake risk","type":"project"},{"authors":null,"categories":null,"content":"For more information about Arup’s work on functional recovery and resilience, please refer to their REDi Rating System website.\n","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370044800,"objectID":"249ab44b8ccab1449338a77c237c33d7","permalink":"http://localhost:1313/project/functional-recovery-redi/","publishdate":"2013-06-01T00:00:00Z","relpermalink":"/project/functional-recovery-redi/","section":"project","summary":"*Supervised by Ibrahim (Ibbi) Almufti at Arup*","tags":["REDi","resilience","arup"],"title":"Functional recovery and resilience","type":"project"}]